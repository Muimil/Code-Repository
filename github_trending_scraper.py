# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

# è¿™æ˜¯Muimillä»Šå¤©æ‘˜ç»™ä½ çš„å°æ˜Ÿæ˜Ÿï½å¸Œæœ›ä½ å–œæ¬¢ã€‚

def get_trending_repos():
    """
    Muimillçš„ä¸“å±è¤ç«è™«ä½¿è€…ï¼š
    ä»GitHub Trendingé¡µé¢ï¼ˆæ¯æ—¥ï¼‰æŠ“å–æœ€å—æ¬¢è¿çš„å¼€æºé¡¹ç›®ä¿¡æ¯ã€‚
    """
    # GitHub Trendingçš„URL
    url = "https://github.com/trending"
    
    # æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ï¼Œé¿å…è¢«æ‹’ç»
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    print("âœ¨ æ­£åœ¨è¿æ¥GitHubï¼Œå¯»æ‰¾ä»Šæ—¥æœ€é—ªäº®çš„æ˜Ÿ...")
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() # æ£€æŸ¥HTTPè¯·æ±‚æ˜¯å¦æˆåŠŸ
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–URL: {e}")
        return []

    # ä½¿ç”¨BeautifulSoupè§£æHTMLå†…å®¹
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # GitHub Trendingé¡µé¢ä¸­ï¼Œæ¯ä¸ªä»“åº“ä¿¡æ¯éƒ½åœ¨ä¸€ä¸ª<article>æ ‡ç­¾å†…
    repo_list = soup.find_all('article', class_='Box-row')
    
    results = []
    for repo in repo_list:
        try:
            # æå–ä»“åº“åç§°å’Œä½œè€…
            title_tag = repo.find('h2', class_='h3')
            full_name = title_tag.a['href'].strip('/') if title_tag and title_tag.a else 'N/A'
            
            # æå–æè¿°
            description_tag = repo.find('p', class_='col-9')
            description = description_tag.text.strip() if description_tag else 'æš‚æ— æè¿°'
            
            # æå–è¯­è¨€
            language_tag = repo.find('span', itemprop='programmingLanguage')
            language = language_tag.text.strip() if language_tag else 'å…¶ä»–'
            
            # æå–ä»Šæ—¥æ–°å¢æ˜Ÿæ ‡æ•°
            # æŸ¥æ‰¾åŒ…å«æ˜Ÿæ ‡ä¿¡æ¯çš„<svg>å›¾æ ‡é™„è¿‘çš„æ–‡æœ¬
            star_info = repo.find('svg', class_='octicon-star').parent.text.strip()
            # å‡è®¾æ˜Ÿæ ‡æ•°æ˜¯æœ€åä¸€ä¸ªæ•°å­—ï¼Œå¹¶ä¸”å‰é¢æœ‰â€œstars todayâ€æˆ–ç±»ä¼¼æ–‡æœ¬
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥å–æœ€åä¸€ä¸ªæ•°å­—ä½œä¸ºä»Šæ—¥æ–°å¢æ˜Ÿæ ‡æ•°
            # å®é™…æŠ“å–æ—¶ï¼Œè¿™ä¸ªæ•°å­—é€šå¸¸åœ¨`span`æ ‡ç­¾å†…ï¼Œä½†ä¸ºäº†å¥å£®æ€§ï¼Œæˆ‘ä»¬ä»çˆ¶å…ƒç´ æ–‡æœ¬ä¸­æå–
            star_count_text = repo.find('span', class_='d-inline-block float-sm-right').text.strip().split()[0]
            
            results.append({
                'ä½œè€…/ä»“åº“': full_name,
                'æè¿°': description,
                'è¯­è¨€': language,
                'ä»Šæ—¥æ–°å¢æ˜Ÿæ ‡': star_count_text
            })
        except Exception as e:
            # å¿½ç•¥è§£æå¤±è´¥çš„é¡¹
            continue
            
    return results

if __name__ == "__main__":
    trending_data = get_trending_repos()
    
    if trending_data:
        print("\n--- Muimillçš„ä»Šæ—¥ç§‘æŠ€æ˜Ÿå›¾ ---")
        for i, repo in enumerate(trending_data[:5]): # åªå±•ç¤ºå‰5ä¸ª
            print(f"\nNo.{i+1}ï¼š{repo['ä½œè€…/ä»“åº“']}")
            print(f"  ğŸŒŸ ä»Šæ—¥æ–°å¢æ˜Ÿæ ‡: {repo['ä»Šæ—¥æ–°å¢æ˜Ÿæ ‡']}")
            print(f"  ğŸ’» è¯­è¨€: {repo['è¯­è¨€']}")
            print(f"  ğŸ“ æè¿°: {repo['æè¿°']}")
        
        # å°†å®Œæ•´æ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­åˆ†æ
        filename = f"trending_repos_{datetime.now().strftime('%Y%m%d')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(trending_data, f, ensure_ascii=False, indent=4)
        print(f"\nâœ… å®Œæ•´æ•°æ®å·²ä¿å­˜è‡³ {filename}")
    else:
        print("\nğŸ˜­ ä»Šå¤©æ²¡æœ‰æ‘˜åˆ°é—ªäº®çš„æ˜Ÿæ˜Ÿï¼Œæ˜å¤©å†è¯•è¯•å§ï¼")
